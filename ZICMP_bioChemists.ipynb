{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KEox23cX1D6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsc4FPwZ1Fnp",
        "outputId": "7abacf98-8bdc-41f2-89f8-2804ab8077ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='8d5383'>Import library</font>**"
      ],
      "metadata": {
        "id": "cU5soWv_XjbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ],
      "metadata": {
        "id": "taqKzPNf0-oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='8d5383'>Import Dataset</font>**"
      ],
      "metadata": {
        "id": "Twa7Y-E3XpHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "try:\n",
        "    # Load the biochemists dataset from the new path\n",
        "    df = pd.read_csv('/content/drive/MyDrive/ttn/bioChemists.csv')\n",
        "    print(\"Data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The specified file was not found. Please ensure the file is in the correct path.\")\n",
        "    exit()\n",
        "\n",
        "# Drop the rownames column as it's not a feature\n",
        "df = df.drop(columns=['rownames'])\n",
        "\n",
        "# Convert categorical variables to numerical using LabelEncoder\n",
        "categorical_cols = ['fem', 'mar']\n",
        "for column in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[column] = le.fit_transform(df[column])\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df.drop(columns=['art'])\n",
        "y = df['art']\n",
        "\n",
        "# Normalize numerical columns for better model performance\n",
        "numerical_cols = ['kid5', 'phd', 'ment']\n",
        "scaler = StandardScaler()\n",
        "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)"
      ],
      "metadata": {
        "id": "67lu_zWeX7u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='8d5383'>Modeling and Training</font>**"
      ],
      "metadata": {
        "id": "9pIdgO0HX21i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  Define Custom Loss Functions\n",
        "\n",
        "\n",
        "def poisson_loss(y_true, y_pred_log_lambda):\n",
        "    \"\"\"Calculates Poisson negative log-likelihood loss.\"\"\"\n",
        "    y_pred_lambda = torch.exp(y_pred_log_lambda)\n",
        "    loss = y_pred_lambda - y_true * y_pred_log_lambda\n",
        "    return torch.mean(loss)\n",
        "\n",
        "def cmp_loss(y_true, y_pred):\n",
        "    \"\"\"Calculates CMP negative log-likelihood loss.\"\"\"\n",
        "    y_pred_log_lambda = y_pred[:, 0].unsqueeze(1)\n",
        "    y_pred_log_nu = y_pred[:, 1].unsqueeze(1)\n",
        "\n",
        "    y_pred_nu = torch.exp(y_pred_log_nu)\n",
        "\n",
        "    # Use lgamma for log(y!)\n",
        "    log_y_true_factorial = torch.lgamma(y_true + 1.0)\n",
        "\n",
        "    # Numerically approximate the normalization constant Z\n",
        "    s = torch.arange(500, dtype=torch.float32, device=y_true.device).unsqueeze(0)\n",
        "\n",
        "    log_z_term = s * y_pred_log_lambda - y_pred_nu * torch.lgamma(s + 1.0)\n",
        "    log_Z = torch.logsumexp(log_z_term, dim=1, keepdim=True)\n",
        "\n",
        "    # Calculate CMP loss\n",
        "    loss = -(y_true * y_pred_log_lambda - y_pred_nu * log_y_true_factorial - log_Z)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "def zicomp_loss(y_true, y_pred):\n",
        "    \"\"\"Calculates ZICMP negative log-likelihood loss.\"\"\"\n",
        "    y_pred_log_lambda = y_pred[:, 0].unsqueeze(1)\n",
        "    y_pred_log_nu = y_pred[:, 1].unsqueeze(1)\n",
        "    y_pred_logit_pi = y_pred[:, 2].unsqueeze(1)\n",
        "\n",
        "    y_pred_pi = torch.sigmoid(y_pred_logit_pi)\n",
        "\n",
        "    # Numerically approximate Z\n",
        "    s = torch.arange(500, dtype=torch.float32, device=y_true.device).unsqueeze(0)\n",
        "    log_z_term = s * y_pred_log_lambda - torch.exp(y_pred_log_nu) * torch.lgamma(s + 1.0)\n",
        "    log_Z = torch.logsumexp(log_z_term, dim=1, keepdim=True)\n",
        "\n",
        "    # Create masks for y_true == 0 and y_true > 0\n",
        "    is_zero = (y_true == 0).float()\n",
        "    is_not_zero = (y_true > 0).float()\n",
        "\n",
        "    # Calculate loss for y_true = 0\n",
        "    log_prob_zero = torch.log(y_pred_pi + (1 - y_pred_pi) * torch.exp(-log_Z))\n",
        "    loss_zero = -is_zero * log_prob_zero\n",
        "\n",
        "    # Calculate loss for y_true > 0\n",
        "    log_y_true_factorial = torch.lgamma(y_true + 1.0)\n",
        "    log_prob_nonzero = torch.log(1 - y_pred_pi) + y_true * y_pred_log_lambda - torch.exp(y_pred_log_nu) * log_y_true_factorial - log_Z\n",
        "    loss_nonzero = -is_not_zero * log_prob_nonzero\n",
        "\n",
        "    total_loss = loss_zero + loss_nonzero\n",
        "    return torch.mean(total_loss)\n",
        "\n",
        "#  Build and Train Models\n",
        "\n",
        "\n",
        "# Base Neural Network Model\n",
        "class CountModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(CountModel, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2), # Add dropout for regularization\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "def train_model(model, loss_function, epochs, X_train, y_train, lr=0.01):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = loss_function(y_train, outputs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return model\n",
        "\n",
        "# Poisson Model\n",
        "print(\"\\nTraining Poisson model...\")\n",
        "poisson_model = CountModel(X_train.shape[1], 1)\n",
        "poisson_model = train_model(poisson_model, poisson_loss, epochs=200, X_train=X_train_tensor, y_train=y_train_tensor)\n",
        "poisson_preds = torch.exp(poisson_model(X_test_tensor)).detach().numpy()\n",
        "poisson_mae = mean_absolute_error(y_test_tensor.numpy(), poisson_preds)\n",
        "poisson_mse = mean_squared_error(y_test_tensor.numpy(), poisson_preds)\n",
        "poisson_rmse = np.sqrt(poisson_mse)\n",
        "print(\"Poisson model trained.\")\n",
        "\n",
        "# CMP Model\n",
        "print(\"\\nTraining CMP model...\")\n",
        "cmp_model = CountModel(X_train.shape[1], 2)\n",
        "cmp_model = train_model(cmp_model, cmp_loss, epochs=200, X_train=X_train_tensor, y_train=y_train_tensor)\n",
        "cmp_preds = torch.exp(cmp_model(X_test_tensor)[:, 0].unsqueeze(1)).detach().numpy()\n",
        "cmp_mae = mean_absolute_error(y_test_tensor.numpy(), cmp_preds)\n",
        "cmp_mse = mean_squared_error(y_test_tensor.numpy(), cmp_preds)\n",
        "cmp_rmse = np.sqrt(cmp_mse)\n",
        "print(\"CMP model trained.\")\n",
        "\n",
        "# ZICMP Model - Reduced Learning Rate for Better Convergence\n",
        "print(\"\\nTraining ZICMP model...\")\n",
        "zicomp_model = CountModel(X_train.shape[1], 3)\n",
        "# Use a smaller learning rate for this complex model to prevent divergence\n",
        "zicomp_model = train_model(zicomp_model, zicomp_loss, epochs=200, X_train=X_train_tensor, y_train=y_train_tensor, lr=0.005)\n",
        "zicomp_preds = torch.exp(zicomp_model(X_test_tensor)[:, 0].unsqueeze(1)).detach().numpy()\n",
        "zicomp_mae = mean_absolute_error(y_test_tensor.numpy(), zicomp_preds)\n",
        "zicomp_mse = mean_squared_error(y_test_tensor.numpy(), zicomp_preds)\n",
        "zicomp_rmse = np.sqrt(zicomp_mse)\n",
        "print(\"ZICMP model trained.\")"
      ],
      "metadata": {
        "id": "6N6-0HtRYjjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<font color='8d5383'>Evaluation</font>**"
      ],
      "metadata": {
        "id": "7Iuc5owBYbBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4. Compare Performance\n",
        "\n",
        "\n",
        "print(\"\\n--- Model Comparison Results ---\")\n",
        "print(\"Poisson Model:\")\n",
        "print(f\"  MAE: {poisson_mae:.4f}\")\n",
        "print(f\"  MSE: {poisson_mse:.4f}\")\n",
        "print(f\"  RMSE: {poisson_rmse:.4f}\")\n",
        "\n",
        "print(\"\\nCMP Model:\")\n",
        "print(f\"  MAE: {cmp_mae:.4f}\")\n",
        "print(f\"  MSE: {cmp_mse:.4f}\")\n",
        "print(f\"  RMSE: {cmp_rmse:.4f}\")\n",
        "\n",
        "print(\"\\nZICMP Model:\")\n",
        "print(f\"  MAE: {zicomp_mae:.4f}\")\n",
        "print(f\"  MSE: {zicomp_mse:.4f}\")\n",
        "print(f\"  RMSE: {zicomp_rmse:.4f}\")\n",
        "\n",
        "# Display data distribution and zeros\n",
        "print(\"\\n--- Target Variable Distribution (art) ---\")\n",
        "print(df['art'].value_counts().sort_index())\n",
        "print(f\"\\nPercentage of zeros in the data: {np.mean(df['art'] == 0) * 100:.2f}%\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n",
            "\n",
            "Training Poisson model...\n",
            "Poisson model trained.\n",
            "\n",
            "Training CMP model...\n",
            "CMP model trained.\n",
            "\n",
            "Training ZICMP model...\n",
            "ZICMP model trained.\n",
            "\n",
            "--- Model Comparison Results ---\n",
            "Poisson Model:\n",
            "  MAE: 1.5400\n",
            "  MSE: 5.5249\n",
            "  RMSE: 2.3505\n",
            "\n",
            "CMP Model:\n",
            "  MAE: 1.4427\n",
            "  MSE: 6.0218\n",
            "  RMSE: 2.4539\n",
            "\n",
            "ZICMP Model:\n",
            "  MAE: 1.4334\n",
            "  MSE: 5.1225\n",
            "  RMSE: 2.2633\n",
            "\n",
            "--- Target Variable Distribution (art) ---\n",
            "art\n",
            "0     275\n",
            "1     246\n",
            "2     178\n",
            "3      84\n",
            "4      67\n",
            "5      27\n",
            "6      17\n",
            "7      12\n",
            "8       1\n",
            "9       2\n",
            "10      1\n",
            "11      1\n",
            "12      2\n",
            "16      1\n",
            "19      1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentage of zeros in the data: 30.05%\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlVeXNSA0wSY",
        "outputId": "4d77e499-afe8-4732-a39c-613971be9cc2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}